{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Zillow Housing data Analysis**\n",
        "\n",
        "**Scraping**\n",
        "1. Use beautiful soup to begin collecting data\n",
        "2. Get 10 observations per state in order to get a general idea of pricing distribution\n",
        "3. Collect necessary information from each property, handling missing values\n",
        "4. Gather price, address, square feet, number of rooms, and number of bathrooms\n",
        "5. Store data for each state in separate dataframes, merging them upon completion of webscraping\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "**Problems faced**\n",
        "Since Zillow's website discourages scraping, we must adapt by delaying time between scrapes and varying user agents. This notebook shows the methods used to scrape the website. To continue building our data, we needed to use multiple computers for scraping using the afformentioned strategy. We also needed to limit 10 requests per request. To account for these issues, we gathered 10 homes from each state and used this data to make estimates of housing information."
      ],
      "metadata": {
        "id": "7nrW9-5IivYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports required**"
      ],
      "metadata": {
        "id": "M-YhHagZiytY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import time"
      ],
      "metadata": {
        "id": "unWmW6cj8b_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**User-Agent**"
      ],
      "metadata": {
        "id": "w4ZQJ7b5i3gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {'User-Agent': 'Launch webscraping project (user-name@gmail.com)'}"
      ],
      "metadata": {
        "id": "T0gwF_ymI-Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Access_link**:\n",
        "- Input: City, State abbreviation, user-agent\n",
        "- Output: A response from the webpage, assuming the request is valid, we should receive a response 200"
      ],
      "metadata": {
        "id": "RYjiLOn0i97i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnrUcLnx7WVo"
      },
      "outputs": [],
      "source": [
        "def access_link (city,state, header_in):\n",
        "  city = city.replace(\" \",\"-\")\n",
        "  url = f\"https://www.zillow.com/homes/{city},-{state}_rb/\"\n",
        "  time.sleep(random.randint(5,16))\n",
        "  response = requests.get(url,headers=header_in)\n",
        "  return response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data collection process**"
      ],
      "metadata": {
        "id": "QytmMKEOjZA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example using NY, NY\n",
        "response = access_link(\"New York\", \"NY\", headers)"
      ],
      "metadata": {
        "id": "2kFV4kQJPPUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = BeautifulSoup(response.text,'html')\n",
        "# get each property card\n",
        "search_results = data.find(\"div\", id=\"grid-search-results\")\n",
        "properties = search_results.find_all(\"li\")"
      ],
      "metadata": {
        "id": "_pp8PqJ7SGs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(properties)\n",
        "for p in properties[:3]:\n",
        "  print(p)"
      ],
      "metadata": {
        "id": "ryH5rLB8Sb5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('html.txt','w') as f:\n",
        "  f.write(response.text)"
      ],
      "metadata": {
        "id": "JFDnTSCRPo6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = access_link(\"New York\", \"NY\", headers)"
      ],
      "metadata": {
        "id": "0UShhSJTJHTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get Prices**:\n",
        "- Input: valid response associated with your initial query\n",
        "- Output: A list of prices associated with homes given the location specified"
      ],
      "metadata": {
        "id": "4X2qIGNlje3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prices(response):\n",
        "  data = BeautifulSoup(response.text,'html')\n",
        "  # find address\n",
        "  time.sleep(random.randint(5,16))\n",
        "  price = data.find_all('div', class_ = \"srp__sc-16e8gqd-0 gKmVGs\")\n",
        "  prices=[]\n",
        "  for i in price:\n",
        "    x = i.text.strip().replace(\"$\",'')\n",
        "    x = x.replace(',','')\n",
        "    prices.append(x)\n",
        "  return prices"
      ],
      "metadata": {
        "id": "yrBmxuHF8T6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_prices(response)"
      ],
      "metadata": {
        "id": "FGU2k9WUJ6FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**get_addresses**\n",
        "- input: A valid response from your desired location\n",
        "- output: Returns a list of addresses associated with the homes in the location specified by the user"
      ],
      "metadata": {
        "id": "BLSiCkh7jurs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_addresses(response):\n",
        "  data = BeautifulSoup(response.text,'html')\n",
        "  time.sleep(random.randint(5,16))\n",
        "  info = data.find_all('div', class_ = \"StyledPropertyCardDataWrapper-c11n-8-85-1__sc-1omp4c3-0 jVBMsP property-card-data\")\n",
        "  lis=[]\n",
        "  for i in info:\n",
        "    time.sleep(random.randint(5,16))\n",
        "    lis.append(i.find('address'))\n",
        "  address=[]\n",
        "  for i in lis:\n",
        "    address.append(i.text.strip())\n",
        "  return address"
      ],
      "metadata": {
        "id": "f25ya2-VAihW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_addresses(response)"
      ],
      "metadata": {
        "id": "iq7mbnBnKAcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**get_housing_info**\n",
        "- input: A valid response from your desired location\n",
        "- output: Returns a dictionary where the keys represent bed, bath, and sqft information associated with each home at the specified location. Each key in the dictionary has a list of the values pulled from the webpage."
      ],
      "metadata": {
        "id": "TPMV2oKYj7cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_housing_info(response):\n",
        "  data = BeautifulSoup(response.text,'html')\n",
        "  time.sleep(random.randint(5,16))\n",
        "  info = data.find_all('b')\n",
        "  lis =[]\n",
        "  count = 0\n",
        "  for i in info:\n",
        "    lis.append(i.text.strip())\n",
        "  dic={}\n",
        "  dic['bed']=[]\n",
        "  dic['bath']=[]\n",
        "  dic['sqft']=[]\n",
        "  for i in range(0,len(lis),3):\n",
        "      dic['bed'].append(lis[i])\n",
        "\n",
        "  for i in range(1,len(lis),3):\n",
        "      dic['bath'].append(lis[i])\n",
        "\n",
        "  for i in range(2,len(lis),3):\n",
        "      dic['sqft'].append(lis[i])\n",
        "  return dic\n"
      ],
      "metadata": {
        "id": "OKeNHLmlCoK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example run with Dover DE\n",
        "response = access_link (\"Dover\",\"DE\", headers)\n",
        "get_housing_info(response)"
      ],
      "metadata": {
        "id": "QI57_Ax2KC4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "372cf990-0fa8-46a4-ed67-1d7e00d58690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bed': ['4', '5', '4', '3', '3', '3', '2', '3', '0.86 acres lot'],\n",
              " 'bath': ['2', '4', '2', '2', '2', '2', '2', '2'],\n",
              " 'sqft': ['2,194',\n",
              "  '3,750',\n",
              "  '2,000',\n",
              "  '1,820',\n",
              "  '1,555',\n",
              "  '1,488',\n",
              "  '1,680',\n",
              "  '1,512']}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scrape_housing**: This function will gather the price, address, number of bedrooms, sqft, and number of bathrooms from each location that the user specifies.\n",
        "- Input: resulting response from access_link function\n",
        "- Output: Returns a dataframe containing all the information associated with homes in the city and state specified by the user. Note, due to the response from the website, each request gets data on the first 10 homes.\n",
        "\n",
        "**Notes**: To reduce issues with responses from Zillow, make sure you do not attempt to pull too much information at once. If you intend to pull more information, some adjustments will need to be made to the script. While time.sleep() is used in the access_link function to reduce issues with Zillow, it is still important to scrape responsibly."
      ],
      "metadata": {
        "id": "m62gU597kWPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_housing(response,state):\n",
        "  prices = get_prices(response)\n",
        "  addresses = get_addresses(response)\n",
        "  dic_info = get_housing_info(response)\n",
        "  dic_info['prices'] = prices\n",
        "  dic_info['addresses'] = addresses\n",
        "  dic_info['cities'] = []\n",
        "  dic_info['state'] = []\n",
        "  for i in addresses:\n",
        "    dic_info['cities'].append(i.split(',')[1])\n",
        "    dic_info['state'].append(state)\n",
        "  df = pd.DataFrame.from_dict(dic_info)\n",
        "  return df"
      ],
      "metadata": {
        "id": "MSEFKAdPDOHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of function\n",
        "df_nyc = scrape_housing(response,\"NY\")"
      ],
      "metadata": {
        "id": "jFWcxJB4NV5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving result to a csv file\n",
        "df_nyc.to_csv(\"nyc.csv\")"
      ],
      "metadata": {
        "id": "kgHGAP6gNo2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pulling state abbreviations**:To keep consistency when choosing cities for each state, we will be using the capitals associated with each state. We also want to pull this information and store it in one location, to make it easier to pass state abbreviations and cities into our functions.\n",
        "**Storing information in dictionary**: We will store this information in a list of tuples, such that the first value is the state abbreviation and the second value is the capital."
      ],
      "metadata": {
        "id": "gWCssvkPlrPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# states and capitals scrape :)\n",
        "url = \"https://bigdave44.com/features/the-mine/us-states-abbreviations-capitals-nicknames/\"\n",
        "response = requests.get(url,headers=headers)"
      ],
      "metadata": {
        "id": "0_Hoy97wUpD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = BeautifulSoup(response.text,'html')"
      ],
      "metadata": {
        "id": "vtcq46JtU2OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lis = []\n",
        "states = data.find_all('tr')\n",
        "for i in states:\n",
        "  lis.append(i.text.strip())\n",
        "states=[]\n",
        "for i in lis[1:51]:\n",
        "  states.append(i.split('\\n')[1])\n",
        "cities=[]\n",
        "for i in lis[1:51]:\n",
        "  cities.append(i.split('\\n')[2])\n",
        "dic_cities = {}\n",
        "dic_cities['state']=states\n",
        "dic_cities['cities']=cities"
      ],
      "metadata": {
        "id": "8tyO-DXpU-O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing capitals and abbreviations in a dataframe\n",
        "cities_df = pd.DataFrame.from_dict(dic_cities)\n",
        "city_info = list(zip(states,cities))\n",
        "city_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAevM7aFXqj8",
        "outputId": "c0a070c9-919b-4b30-c3a7-0b0b9809a086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('AL', 'Montgomery'),\n",
              " ('AK', 'Juneau'),\n",
              " ('AZ', 'Phoenix'),\n",
              " ('AR', 'Little Rock'),\n",
              " ('CA', 'Sacramento'),\n",
              " ('CO', 'Denver'),\n",
              " ('CT', 'Hartford'),\n",
              " ('DE', 'Dover'),\n",
              " ('FL', 'Tallahassee'),\n",
              " ('GA', 'Atlanta'),\n",
              " ('HI', 'Honolulu'),\n",
              " ('ID', 'Boise'),\n",
              " ('IL', 'Springfield'),\n",
              " ('IN', 'Indianapolis'),\n",
              " ('IA', 'Des Moines'),\n",
              " ('KS', 'Topeka'),\n",
              " ('KY', 'Frankfort'),\n",
              " ('LA', 'Baton Rouge'),\n",
              " ('ME', 'Augusta'),\n",
              " ('MD', 'Annapolis'),\n",
              " ('MA', 'Boston'),\n",
              " ('MI', 'Lansing'),\n",
              " ('MN', 'St. Paul'),\n",
              " ('MS', 'Jackson'),\n",
              " ('MO', 'Jefferson City'),\n",
              " ('MT', 'Helena'),\n",
              " ('NE', 'Lincoln'),\n",
              " ('NV', 'Carson City'),\n",
              " ('NH', 'Concord'),\n",
              " ('NJ', 'Trenton'),\n",
              " ('NM', 'Santa Fe'),\n",
              " ('NY', 'Albany'),\n",
              " ('NC', 'Raleigh'),\n",
              " ('ND', 'Bismarck'),\n",
              " ('OH', 'Columbus'),\n",
              " ('OK', 'Oklahoma City'),\n",
              " ('OR', 'Salem'),\n",
              " ('PA', 'Harrisburg'),\n",
              " ('RI', 'Providence'),\n",
              " ('SC', 'Columbia'),\n",
              " ('SD', 'Pierre'),\n",
              " ('TN', 'Nashville'),\n",
              " ('TX', 'Austin'),\n",
              " ('UT', 'Salt Lake City'),\n",
              " ('VT', 'Montpelier'),\n",
              " ('VA', 'Richmond'),\n",
              " ('WA', 'Olympia'),\n",
              " ('WV', 'Charleston'),\n",
              " ('WI', 'Madison'),\n",
              " ('WY', 'Cheyenne')]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing information from Hawaii\n",
        "response_hi = access_link (\"HI\",\"Hilo\", headers)\n",
        "dic_hi = scrape_housing(response_hi,\"HI\")\n",
        "df = pd.DataFrame.from_dict(dic_info)\n"
      ],
      "metadata": {
        "id": "8D6WNPzTnbPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_hi\n",
        "dic_de['bed'] = ['4', '5', '4', '3', '3', '3', '2', '3', '--']\n",
        "dic_de['bath'] = ['2', '4', '2', '2', '2', '2', '2', '2','--']\n",
        "dic_de['sqft'] = ['2,194',\n",
        "  '3,750',\n",
        "  '2,000',\n",
        "  '1,820',\n",
        "  '1,555',\n",
        "  '1,488',\n",
        "  '1,680',\n",
        "  '1,512', '--']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuV5EtJhCkLg",
        "outputId": "14e8500f-6415-4ef4-cd6b-0e63c9426134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bed': ['5', '3', '0.53 acres lot', '2', '3', '3', '6'],\n",
              " 'bath': ['3', '3', '0.54 acres lot', '1,212', '2,664', '2', '3'],\n",
              " 'sqft': ['2,202', '1,862', '3', '4', '0.43 acres lot', '1,355', '1,600'],\n",
              " 'prices': ['770000',\n",
              "  '719000',\n",
              "  '249000',\n",
              "  '330000',\n",
              "  '1199000',\n",
              "  '1999995',\n",
              "  '649000',\n",
              "  '850000',\n",
              "  '680000'],\n",
              " 'addresses': ['50 Malia St, Hilo, HI 96720',\n",
              "  '1172 Kumukoa St, Hilo, HI 96720',\n",
              "  '1139 Kaumana Dr, Hilo, HI 96720',\n",
              "  '3914 Kila Pl LOT 6-C, Hilo, HI 96720',\n",
              "  '2048 Kalanianaole St, Hilo, HI 96720',\n",
              "  '229 Maikai St, Hilo, HI 96720',\n",
              "  '106 Nene St, Hilo, HI 96720',\n",
              "  '30 Koula St, Hilo, HI 96720',\n",
              "  '1280 Kumukoa St, Hilo, HI 96720'],\n",
              " 'cities': [' Hilo',\n",
              "  ' Hilo',\n",
              "  ' Hilo',\n",
              "  ' Hilo',\n",
              "  ' Hilo',\n",
              "  ' Hilo',\n",
              "  ' Hilo',\n",
              "  ' Hilo',\n",
              "  ' Hilo'],\n",
              " 'state': ['HI', 'HI', 'HI', 'HI', 'HI', 'HI', 'HI', 'HI', 'HI']}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combining states**: The initial states gathered have been stored in in separate csv files. In order to begin our analysis, we must combine the dataframes. The remaining states and their corresponding data was collected by my partners."
      ],
      "metadata": {
        "id": "3zpEFs-LmyoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ct = pd.DataFrame.from_dict(dic_ct)\n",
        "df_al = pd.read_csv(\"alabama.csv\")\n",
        "df_alas = pd.read_csv(\"alaska.csv\")\n",
        "df_ar = pd.read_csv(\"arizona.csv\")\n",
        "df_cali = pd.read_csv(\"cali.csv\")\n",
        "df_col = pd.read_csv(\"coloroo.csv\")\n",
        "df_geo = pd.read_csv(\"georgia.csv\")\n",
        "df_indi = pd.read_csv(\"indiana.csv\")\n",
        "df_ka = pd.read_csv(\"kansas.csv\")\n",
        "df_nyc = pd.read_csv(\"nyc.csv\")\n",
        "\n",
        "lis = [df_al,df_alas,df_ar,df_cali,df_col,df_geo,df_nyc, df_ct]\n",
        "df = pd.concat(lis)\n",
        "df_az = pd.DataFrame.from_dict(dic_ark)\n",
        "df_az.to_csv(\"arizona.csv\")"
      ],
      "metadata": {
        "id": "yh8yaLq_8Uhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing = pd.read_csv('housing.csv')\n",
        "df.drop(df.columns[[0, 1]], axis=1, inplace=True)\n",
        "df.to_csv('housing.csv')"
      ],
      "metadata": {
        "id": "ejo_H8PGDa1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv('housing.csv')\n",
        "lis = [housing,df]\n",
        "df = pd.concat(lis)\n",
        "df.drop(df.columns[[0,1]],axis =1, inplace = True)"
      ],
      "metadata": {
        "id": "LCyxGKyxEKQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"housing.csv\")\n",
        "pd.DataFrame.from_dict(df_ct)\n",
        "colorodo_df.to_csv(\"coloroo.csv\")\n",
        "df.to_csv('housing.csv')"
      ],
      "metadata": {
        "id": "8sbRga9aos0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Access_link (alternate)**: Due to issues with getting requests from Zillow, an alternate access_link() function was written. This function varies the User-Agent in the request, and increases the time the program takes between requests. This function was used to gather the final few states since we ran into more issues while gathering data."
      ],
      "metadata": {
        "id": "bInGvlJYnUjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "access_link (city,state, header_in)\n",
        "scrape_housing(response,state)\n",
        "lis =[]\n",
        "count = 0\n",
        "for i in city_info[2:17]:\n",
        "  count +=1\n",
        "  if count ==5:\n",
        "    time.sleep(random.randint(10,25))\n",
        "  elif count %2==0:\n",
        "    headers = {'User-Agent': 'hello-world@yahoo.com'}\n",
        "    response = access_link (i[1],i[0], headers)\n",
        "    dic = scrape_housing(response,i[0])\n",
        "    lis.append(dic)\n",
        "  elif count %3==0:\n",
        "    headers = {'User-Agent': 'dinosaur@gmail.com'}\n",
        "    response = access_link (i[1],i[0], headers)\n",
        "    dic = scrape_housing(response,i[0])\n",
        "    lis.append(dic)\n",
        "  elif count %4==0:\n",
        "    headers = {'User-Agent': 'long-neck@yahoo.com'}\n",
        "    response = access_link (i[1],i[0], headers)\n",
        "    dic = scrape_housing(response,i[0])\n",
        "    lis.append(dic)\n",
        "  elif count %5==0:\n",
        "    headers = {'User-Agent': 'bread-winter@gmail.com'}\n",
        "    response = access_link (i[1],i[0], headers)\n",
        "    dic = scrape_housing(response,i[0])\n",
        "    lis.append(dic)\n",
        "  else:\n",
        "    headers = {'User-Agent': 'goodbye-world@gmail.com'}\n",
        "    response = access_link (i[1],i[0], headers)\n",
        "    dic = scrape_housing(response,i[0])\n",
        "    lis.append(dic)"
      ],
      "metadata": {
        "id": "9ELTc_UBYRrV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}